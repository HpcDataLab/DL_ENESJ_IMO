{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version is: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensorflow version is: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"../../data/OCT/OCT2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Classes Detected : 4\n"
     ]
    }
   ],
   "source": [
    "labels_available = os.listdir(os.path.join(dataPath, \"train\"))\n",
    "print(\"Total Number of Classes Detected :\",len(labels_available))\n",
    "\n",
    "labels_list = ['NORMAL',\"CNV\",\"DME\",\"DRUSEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL CNV DME DRUSEN "
     ]
    }
   ],
   "source": [
    "images=[]\n",
    "y_trn=[]\n",
    "for x in labels_list:\n",
    "    xPath = os.path.join(dataPath, \"train\", x)\n",
    "    myPicList = os.listdir(xPath)\n",
    "    for y in myPicList:\n",
    "        images.append(cv2.imread(os.path.join(xPath, y)))\n",
    "        y_trn.append(labels_list.index(x))\n",
    "    print(x ,end=\" \")\n",
    "\n",
    "y_train = to_categorical(y_trn,len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL CNV DME DRUSEN "
     ]
    }
   ],
   "source": [
    "x_val=[]\n",
    "y_val=[]\n",
    "for x in labels_list:\n",
    "    xPath = os.path.join(dataPath, \"val\", x)\n",
    "    myPicList = os.listdir(xPath)\n",
    "    for y in myPicList:\n",
    "        x_val.append(cv2.imread(os.path.join(xPath, y)))\n",
    "        y_val.append(labels_list.index(x))\n",
    "    print(x ,end=\" \")\n",
    "\n",
    "y_validation = to_categorical(y_val,len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL CNV DME DRUSEN "
     ]
    }
   ],
   "source": [
    "x_tst=[]\n",
    "y_tst=[]\n",
    "for x in labels_list:\n",
    "    xPath = os.path.join(dataPath, \"test\", x)\n",
    "    myPicList = os.listdir(xPath)\n",
    "    for y in myPicList:\n",
    "        x_tst.append(cv2.imread(os.path.join(xPath, y)))\n",
    "        y_tst.append(labels_list.index(x))\n",
    "    print(x ,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of images per category and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\t\tTrain\t\tValidation\t\tTest\n",
      "NORMAL\t\t26315\t\t8\t\t\t242\n",
      "CNV\t\t37205\t\t8\t\t\t242\n",
      "DME\t\t11348\t\t8\t\t\t242\n",
      "DRUSEN\t\t8616\t\t8\t\t\t242\n"
     ]
    }
   ],
   "source": [
    "def getNumberOfItems(labels, itemLabel):\n",
    "    return sum(list(map(lambda x: x == itemLabel, labels))) \n",
    "\n",
    "\n",
    "print(\"Label\\t\\tTrain\\t\\tValidation\\t\\tTest\")\n",
    "for i, label in enumerate(labels_list):\n",
    "    print(f\"{label}\\t\\t{getNumberOfItems(y_trn, i)}\\t\\t{getNumberOfItems(y_val, i)}\\t\\t\\t{getNumberOfItems(y_tst, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeIm(im, size):\n",
    "    if im.shape[2] == 1:\n",
    "        im = cv2.cvtColor(im,cv2.COLOR_GRAY2RGB)\n",
    "    return cv2.resize(im, size)\n",
    "\n",
    "def resizeIms(x, size):\n",
    "    return np.array(list(map(lambda im: resizeIm(im, size), x)))\n",
    "\n",
    "# Substract the mean specified by Mean (https://www.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.imageinputlayer.html)\n",
    "# Substract mean by channel\n",
    "def zerocenter(x):\n",
    "    return (x - np.mean(x, axis=(0, 1))) / (2*np.std(x, axis=(0, 1)))\n",
    "\n",
    "# Rescale the input to be in the range [-1, 1] using the minimum and maximum values specified by Min and Max, respectively. (https://www.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.imageinputlayer.html)\n",
    "# Compute using the formula: $x\\prime = 2 * \\frac{x - \\min{x}}{\\max{x} - \\min{x}} - 1$ taken from: https://stats.stackexchange.com/a/178629\n",
    "def rescaleSymmetric(x):\n",
    "    x = zerocenter(x)\n",
    "    return 2 * (x - np.min(x, axis=(0, 1)))/np.ptp(x, axis=(0, 1)) - 1\n",
    "\n",
    "def normalizeIms(x, normalization):\n",
    "    if normalization == \"zerocenter\":\n",
    "        func = zerocenter\n",
    "    elif normalization == \"rescale-symmetric\":\n",
    "        func = rescaleSymmetric\n",
    "    else:\n",
    "        return x\n",
    "    return np.array(list(map(lambda im: func(im), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,fraction, model):\n",
    "        super(CustomCallback,self).__init__()\n",
    "        self.fraction = fraction\n",
    "        self.train_a = [];\n",
    "        self.val_a =[];\n",
    "        self.logPath = os.path.join(model, \"log.txt\")\n",
    "\n",
    "        if not os.path.isdir(model):\n",
    "            os.mkdir(model)\n",
    "\n",
    "        with open(self.logPath,'w') as f:\n",
    "            f.write('Starting of logging..\\n')\n",
    "\n",
    "        self.fig = plt.figure(figsize=(4,3))\n",
    "        self.ax = plt.subplot(1,1,1)\n",
    "        plt.ion()\n",
    "\n",
    "    def on_train_begin(self,logs=None):\n",
    "        self.fig.show()\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def on_train_end(self,logs=None):\n",
    "        with open(self.logPath,'a') as f:\n",
    "              f.write('End of logging..\\n')\n",
    "    \n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        lr= tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        lr *= self.fraction\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr,lr)\n",
    "        with open(self.logPath,'a') as f:\n",
    "            f.write('At epoch {:02d}, learning rate changed to {:.4f}\\n'.format(epoch,lr))\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        val_acc = logs.get('val_accuracy')\n",
    "        train_acc = logs.get('accuracy')\n",
    "        self.train_a.append(train_acc)\n",
    "        self.val_a.append(val_acc)\n",
    "        with open(self.logPath,'a') as f:\n",
    "            f.write('At epoch {:02d}, training accuracy: {:.3f}, validation accuracy: {:.3f}\\n'.format(epoch,train_acc,val_acc))\n",
    "        self.ax.clear()\n",
    "        self.ax.plot(range(1+epoch),self.train_a,label=\"Training\")\n",
    "        self.ax.plot(range(1+epoch),self.val_a,label=\"Validation\")\n",
    "        self.ax.set_xlabel('Epochs')\n",
    "        self.ax.set_ylabel('Accuracy')\n",
    "        self.ax.legend()\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.show()\n",
    "        \n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# xception base model\n",
    "#from tensorflow.keras.applications.xception import Xception\n",
    "#from tensorflow.keras.applications.xception import preprocess_input, decode_predictions\n",
    "#xceptionNetModel = Xception(weights='imagenet')\n",
    "#xceptionNetModel.save(\"../Xception.hdf5\")\n",
    "\n",
    "xceptionNetModel = tf.keras.models.load_model('../Xception.hdf5')\n",
    "\n",
    "# opticnet base model\n",
    "opticNetModel = tf.keras.models.load_model('../Optic_net-4_classes-Kermany2018.hdf5')\n",
    "\n",
    "# resnet base model\n",
    "#from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "#from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#resNetModel = ResNet50(weights='imagenet')\n",
    "#resNetModel.save(\"../Resnet50.hdf5\")\n",
    "\n",
    "resNetModel = tf.keras.models.load_model('../Resnet50.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 221, 221, 32)      4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 221, 221, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 104, 104, 32)      50208     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 104, 104, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 52, 52, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 743,812\n",
      "Trainable params: 742,788\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_init = tf.keras.initializers.RandomNormal(mean=0., stddev=1.) # convolutional layer with zero mean and 1 sd\n",
    "#batch_init = tf.keras.initializers.RandomNormal(mean=1., stddev=0.02) # non-sense here\n",
    "\n",
    "octnet = keras.models.Sequential() # empty model\n",
    "\n",
    "octnet.add(keras.layers.Conv2D(32, (7, 7), activation=\"relu\", use_bias=True, input_shape=(227, 227, 3), kernel_initializer=conv_init))\n",
    "octnet.add(keras.layers.BatchNormalization())\n",
    "octnet.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "octnet.add(keras.layers.Conv2D(32, (7, 7), activation=\"relu\", use_bias=True, kernel_initializer=conv_init))\n",
    "octnet.add(keras.layers.BatchNormalization())\n",
    "octnet.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "octnet.add(keras.layers.Conv2D(64, (5, 5), activation=\"relu\", use_bias=True, kernel_initializer=conv_init))\n",
    "octnet.add(keras.layers.BatchNormalization())\n",
    "octnet.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "octnet.add(keras.layers.Conv2D(128, (5, 5), activation=\"relu\", use_bias=True, kernel_initializer=conv_init))\n",
    "octnet.add(keras.layers.BatchNormalization())\n",
    "octnet.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "octnet.add(keras.layers.Conv2D(256, (3, 3), activation=\"relu\", use_bias=True, kernel_initializer=conv_init))\n",
    "octnet.add(keras.layers.BatchNormalization())\n",
    "octnet.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#octnet.add(keras.layers.Conv2D(512, (3, 3), activation=\"relu\", use_bias=True))\n",
    "#octnet.add(keras.layers.BatchNormalization())\n",
    "#octnet.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "octnet.add(keras.layers.AveragePooling2D())\n",
    "octnet.add(keras.layers.Flatten())\n",
    "octnet.add(keras.layers.Dense(128))\n",
    "octnet.add(keras.layers.Dropout(0.5))\n",
    "octnet.add(keras.layers.Dense(32))\n",
    "octnet.add(keras.layers.Dropout(0.5))\n",
    "octnet.add(keras.layers.Dense(4))\n",
    "octnet.add(keras.layers.Softmax())\n",
    "\n",
    "octnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyModelGenerator(model, newWeights=False, lastOnly=False):\n",
    "    model = model.lower() # lower case model name\n",
    "    if model == \"resnet\":\n",
    "        x = resNetModel.output\n",
    "        predictions = Dense(len(labels_list), activation='softmax')(x)\n",
    "        newModel = Model(inputs=resNetModel.input, outputs=predictions)\n",
    "        size = (224, 224)\n",
    "        normalization = \"zerocenter\"\n",
    "    elif model == \"xception\":\n",
    "        x = xceptionNetModel.output\n",
    "        predictions = Dense(len(labels_list), activation='softmax')(x)\n",
    "        newModel = Model(inputs=xceptionNetModel.input, outputs=predictions)\n",
    "        size = (299, 299)\n",
    "        normalization = \"rescale-symmetric\"\n",
    "    elif model == \"opticnet\":\n",
    "        newModel = tf.keras.models.load_model('../Optic_net-4_classes-Kermany2018.hdf5')\n",
    "        size = (224, 224)\n",
    "        normalization = \"rescale-symmetric\"\n",
    "    elif model == \"octnet\":\n",
    "        newModel = tf.keras.models.clone_model(octnet)\n",
    "        size = (227, 227)\n",
    "        normalization = \"zerocenter\"\n",
    "    if newWeights:\n",
    "        newModel = tf.keras.models.clone_model(newModel)\n",
    "    if lastOnly:\n",
    "        newModel = lastDenseTrainable(newModel) # block all layer except the last dense ones\n",
    "    else:\n",
    "        newModel.trainable = True\n",
    "    return newModel, size, normalization\n",
    "\n",
    "\n",
    "def lastDenseTrainable(model):\n",
    "    model.trainable = False\n",
    "    for i in range(1, len(model.layers)):\n",
    "        if model.layers[-i].__class__.__name__ == \"Dense\":\n",
    "            model.layers[-i].trainable= True\n",
    "        elif model.layers[-i].__class__.__name__ == \"Activation\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return model\n",
    "    \n",
    "\n",
    "def testPredict(model, size, name=None, normalization=None):\n",
    "    X_test = resizeIms(x_tst, size)\n",
    "    if normalization:\n",
    "        X_test = normalizeIms(X_test, normalization)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(y_tst)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    preds = np.argmax(prediction, axis=1)\n",
    "    acc = sum(preds == Y_test) / len(Y_test)\n",
    "    print(f'Test acc for {name if name else \"model\"}: {acc:.6f}')\n",
    "    return acc\n",
    "    \n",
    "\n",
    "def computeConfussionMatrix(predictions, labels):\n",
    "    num_labels = len(labels_list)\n",
    "    cMatrix = np.zeros(shape=(num_labels, num_labels))\n",
    "    for i in len(predictions):\n",
    "        p = int(predictions[i])\n",
    "        t = int(predictions[i])\n",
    "        cMatrix[t, p] += 1\n",
    "    print(cMatrix)\n",
    "    ax = sns.heatmap(cMatrix, cmap=\"Blues\", annot=True, fmt=\"d\", xticklabels=labels_list, yticklabels=labels_list)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.show();\n",
    "    return cMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**************************************************\n",
      "Model: xception\n",
      "**************************************************\n",
      "\n",
      "Total: 135 trainable: 2\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Model: resnet\n",
      "**************************************************\n",
      "\n",
      "Total: 178 trainable: 2\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Model: opticnet\n",
      "**************************************************\n",
      "\n",
      "Total: 276 trainable: 2\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Model: octnet\n",
      "**************************************************\n",
      "\n",
      "Total: 23 trainable: 0\n"
     ]
    }
   ],
   "source": [
    "models = [\"xception\", \"resnet\", \"opticnet\", \"octnet\"]\n",
    "\n",
    "def printTrainableLayers(m):\n",
    "    a = 0\n",
    "    for l in m.layers:\n",
    "        #l.trainable = True\n",
    "        if l.trainable:\n",
    "            #print(l.name, l.trainable)\n",
    "            a += 1\n",
    "    print(f\"\\nTotal: {len(m.layers)} trainable: {a}\")\n",
    "\n",
    "for mod in models:\n",
    "    print(\"\\n\\n\" + (\"*\"*50) + \"\\nModel: \" + mod + '\\n' + (\"*\"*50))\n",
    "    m = emptyModelGenerator(mod, newWeights=False, lastOnly=True)[0]\n",
    "    printTrainableLayers(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "optim = lambda lr: Adam(learning_rate=lr)\n",
    "#optim = lambda lr, momentum: SGD(learning_rate=lr, momentum=momentum)\n",
    "epochs = 30\n",
    "batch_size = 50\n",
    "#batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv(\"Results_temp.csv\")\n",
    "results[\"normalization\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#res = pd.read_csv(\"Results_temp.csv\")\n",
    "#res.to_csv(\"Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels fraction: [0.0100703  0.01042871 0.00845964 0.00986537]\n",
      "Training resnet for 0.01% of train size (aka 834 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 6/17 [=========>....................] - ETA: 8s - loss: 1.3761 - accuracy: 0.3281 WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1026s vs `on_train_batch_begin` time: 0.1445s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1026s vs `on_train_batch_end` time: 0.3820s). Check your callbacks.\n",
      "17/17 [==============================] - 14s 453ms/step - loss: 1.3370 - accuracy: 0.4057 - val_loss: 1.4584 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 1.2283 - accuracy: 0.4667 - val_loss: 1.5153 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 1.1944 - accuracy: 0.4784 - val_loss: 1.5394 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 3s 194ms/step - loss: 1.2225 - accuracy: 0.4547 - val_loss: 1.5533 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 3s 195ms/step - loss: 1.2027 - accuracy: 0.4589 - val_loss: 1.5663 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 3s 195ms/step - loss: 1.2149 - accuracy: 0.4593 - val_loss: 1.5684 - val_accuracy: 0.2500\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 3s 194ms/step - loss: 1.2209 - accuracy: 0.4505 - val_loss: 1.5735 - val_accuracy: 0.2500\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 1.2271 - accuracy: 0.4425 - val_loss: 1.5766 - val_accuracy: 0.2500\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 3s 195ms/step - loss: 1.1762 - accuracy: 0.4880 - val_loss: 1.5746 - val_accuracy: 0.2500\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 3s 195ms/step - loss: 1.1891 - accuracy: 0.4772 - val_loss: 1.5744 - val_accuracy: 0.2500\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 3s 195ms/step - loss: 1.1888 - accuracy: 0.4715 - val_loss: 1.5747 - val_accuracy: 0.2500\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 1.1784 - accuracy: 0.4571 - val_loss: 1.5758 - val_accuracy: 0.2500\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 1.2301 - accuracy: 0.4607 - val_loss: 1.5764 - val_accuracy: 0.2500\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 1.2049 - accuracy: 0.4579 - val_loss: 1.5766 - val_accuracy: 0.2500\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 1.1941 - accuracy: 0.4555 - val_loss: 1.5780 - val_accuracy: 0.2500\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 1.2204 - accuracy: 0.4668 - val_loss: 1.5773 - val_accuracy: 0.2500\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 1.1884 - accuracy: 0.4702 - val_loss: 1.5782 - val_accuracy: 0.2500\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 1.2103 - accuracy: 0.4581 - val_loss: 1.5774 - val_accuracy: 0.2500\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 1.1882 - accuracy: 0.4663 - val_loss: 1.5788 - val_accuracy: 0.2500\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 1.1948 - accuracy: 0.4608 - val_loss: 1.5794 - val_accuracy: 0.2500\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.1935 - accuracy: 0.4644 - val_loss: 1.5788 - val_accuracy: 0.2500\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.1953 - accuracy: 0.4584 - val_loss: 1.5798 - val_accuracy: 0.2500\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.2021 - accuracy: 0.4791 - val_loss: 1.5797 - val_accuracy: 0.2500\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.1899 - accuracy: 0.4549 - val_loss: 1.5793 - val_accuracy: 0.2500\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.1708 - accuracy: 0.4605 - val_loss: 1.5815 - val_accuracy: 0.2500\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.1991 - accuracy: 0.4768 - val_loss: 1.5823 - val_accuracy: 0.2500\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.2184 - accuracy: 0.4568 - val_loss: 1.5843 - val_accuracy: 0.2500\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.2137 - accuracy: 0.4542 - val_loss: 1.5858 - val_accuracy: 0.2500\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.1963 - accuracy: 0.4413 - val_loss: 1.5888 - val_accuracy: 0.2500\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 1.1720 - accuracy: 0.4619 - val_loss: 1.5894 - val_accuracy: 0.2500\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_835_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.250000\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.02496675 0.02607176 0.02264716 0.02356082]\n",
      "Training resnet for 0.025% of train size (aka 2087 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 6/42 [===>..........................] - ETA: 26s - loss: 1.3736 - accuracy: 0.2863WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1041s vs `on_train_batch_begin` time: 0.1423s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1041s vs `on_train_batch_end` time: 0.3834s). Check your callbacks.\n",
      "42/42 [==============================] - 20s 296ms/step - loss: 1.2978 - accuracy: 0.4218 - val_loss: 1.5347 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "42/42 [==============================] - 8s 196ms/step - loss: 1.2181 - accuracy: 0.4676 - val_loss: 1.5745 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 8s 196ms/step - loss: 1.2122 - accuracy: 0.4677 - val_loss: 1.5814 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 8s 197ms/step - loss: 1.1950 - accuracy: 0.4753 - val_loss: 1.5731 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 8s 198ms/step - loss: 1.1923 - accuracy: 0.4446 - val_loss: 1.5618 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 8s 198ms/step - loss: 1.0775 - accuracy: 0.5023 - val_loss: 1.5400 - val_accuracy: 0.2812\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 8s 198ms/step - loss: 0.9896 - accuracy: 0.7125 - val_loss: 1.5517 - val_accuracy: 0.2500\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.9189 - accuracy: 0.7214 - val_loss: 1.5990 - val_accuracy: 0.2188\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 8s 198ms/step - loss: 0.8863 - accuracy: 0.7358 - val_loss: 1.5793 - val_accuracy: 0.2500\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.8602 - accuracy: 0.7314 - val_loss: 1.9422 - val_accuracy: 0.2812\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.8223 - accuracy: 0.7424 - val_loss: 1.9137 - val_accuracy: 0.2812\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.8195 - accuracy: 0.7327 - val_loss: 2.1309 - val_accuracy: 0.2500\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.7862 - accuracy: 0.7547 - val_loss: 1.3511 - val_accuracy: 0.3750\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 8s 198ms/step - loss: 0.7573 - accuracy: 0.7513 - val_loss: 1.3834 - val_accuracy: 0.5000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.7039 - accuracy: 0.7685 - val_loss: 1.3855 - val_accuracy: 0.5000\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.6604 - accuracy: 0.7842 - val_loss: 1.1895 - val_accuracy: 0.5000\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.6534 - accuracy: 0.7856 - val_loss: 1.1891 - val_accuracy: 0.5000\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.6248 - accuracy: 0.7864 - val_loss: 1.1541 - val_accuracy: 0.5000\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.6308 - accuracy: 0.7669 - val_loss: 1.1682 - val_accuracy: 0.5000\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.6116 - accuracy: 0.7744 - val_loss: 1.1411 - val_accuracy: 0.5000\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.5973 - accuracy: 0.7683 - val_loss: 1.2959 - val_accuracy: 0.5000\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.5232 - accuracy: 0.7945 - val_loss: 1.0612 - val_accuracy: 0.5000\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.5321 - accuracy: 0.7766 - val_loss: 1.7121 - val_accuracy: 0.4375\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.5124 - accuracy: 0.7805 - val_loss: 1.2374 - val_accuracy: 0.4688\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.4889 - accuracy: 0.7870 - val_loss: 1.1004 - val_accuracy: 0.5312\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.4842 - accuracy: 0.8297 - val_loss: 1.0325 - val_accuracy: 0.6250\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.4795 - accuracy: 0.8544 - val_loss: 1.0035 - val_accuracy: 0.6250\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.4536 - accuracy: 0.8740 - val_loss: 1.1705 - val_accuracy: 0.6562\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.4588 - accuracy: 0.8736 - val_loss: 0.9826 - val_accuracy: 0.6562\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 8s 199ms/step - loss: 0.4412 - accuracy: 0.8856 - val_loss: 0.9559 - val_accuracy: 0.6250\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_2087_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.658058\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.05050352 0.04929445 0.05137469 0.04967502]\n",
      "Training resnet for 0.05% of train size (aka 4174 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 6/84 [=>............................] - ETA: 54s - loss: 1.3762 - accuracy: 0.3329 WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_begin` time: 0.1427s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.3654s). Check your callbacks.\n",
      "84/84 [==============================] - 26s 243ms/step - loss: 1.2900 - accuracy: 0.4205 - val_loss: 1.5317 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "84/84 [==============================] - 16s 196ms/step - loss: 1.2388 - accuracy: 0.4401 - val_loss: 1.5276 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "84/84 [==============================] - 17s 197ms/step - loss: 1.2431 - accuracy: 0.4349 - val_loss: 1.5486 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "84/84 [==============================] - 17s 197ms/step - loss: 1.1976 - accuracy: 0.5258 - val_loss: 1.6204 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "84/84 [==============================] - 17s 197ms/step - loss: 1.0003 - accuracy: 0.6866 - val_loss: 1.6946 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.8738 - accuracy: 0.7212 - val_loss: 1.7022 - val_accuracy: 0.2812\n",
      "Epoch 7/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.8197 - accuracy: 0.7221 - val_loss: 1.8566 - val_accuracy: 0.2500\n",
      "Epoch 8/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.8067 - accuracy: 0.7196 - val_loss: 1.9411 - val_accuracy: 0.2500\n",
      "Epoch 9/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.8063 - accuracy: 0.7072 - val_loss: 1.0682 - val_accuracy: 0.5000\n",
      "Epoch 10/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.7412 - accuracy: 0.7307 - val_loss: 1.0850 - val_accuracy: 0.5000\n",
      "Epoch 11/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.7028 - accuracy: 0.7310 - val_loss: 1.1669 - val_accuracy: 0.5000\n",
      "Epoch 12/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.6600 - accuracy: 0.7508 - val_loss: 1.0490 - val_accuracy: 0.5312\n",
      "Epoch 13/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.6247 - accuracy: 0.7640 - val_loss: 1.1233 - val_accuracy: 0.5000\n",
      "Epoch 14/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.6142 - accuracy: 0.7628 - val_loss: 1.0065 - val_accuracy: 0.5000\n",
      "Epoch 15/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.6109 - accuracy: 0.7651 - val_loss: 1.0895 - val_accuracy: 0.5625\n",
      "Epoch 16/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.6145 - accuracy: 0.8012 - val_loss: 0.8283 - val_accuracy: 0.7188\n",
      "Epoch 17/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.5497 - accuracy: 0.8318 - val_loss: 1.0778 - val_accuracy: 0.6250\n",
      "Epoch 18/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.5455 - accuracy: 0.8187 - val_loss: 1.0348 - val_accuracy: 0.6875\n",
      "Epoch 19/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.5083 - accuracy: 0.8429 - val_loss: 1.2191 - val_accuracy: 0.6250\n",
      "Epoch 20/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.4728 - accuracy: 0.8511 - val_loss: 1.4353 - val_accuracy: 0.5000\n",
      "Epoch 21/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.4623 - accuracy: 0.8446 - val_loss: 1.0004 - val_accuracy: 0.6562\n",
      "Epoch 22/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.4380 - accuracy: 0.8546 - val_loss: 0.8815 - val_accuracy: 0.7188\n",
      "Epoch 23/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.4186 - accuracy: 0.8620 - val_loss: 0.9983 - val_accuracy: 0.6250\n",
      "Epoch 24/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.4061 - accuracy: 0.8607 - val_loss: 1.0289 - val_accuracy: 0.5938\n",
      "Epoch 25/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.3749 - accuracy: 0.8769 - val_loss: 1.1627 - val_accuracy: 0.5312\n",
      "Epoch 26/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.3817 - accuracy: 0.8722 - val_loss: 0.9260 - val_accuracy: 0.6562\n",
      "Epoch 27/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.3471 - accuracy: 0.8841 - val_loss: 0.9313 - val_accuracy: 0.6875\n",
      "Epoch 28/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.3211 - accuracy: 0.8871 - val_loss: 0.8649 - val_accuracy: 0.6250\n",
      "Epoch 29/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.3367 - accuracy: 0.8813 - val_loss: 0.6195 - val_accuracy: 0.7188\n",
      "Epoch 30/30\n",
      "84/84 [==============================] - 17s 198ms/step - loss: 0.3270 - accuracy: 0.8792 - val_loss: 1.0949 - val_accuracy: 0.6250\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_4174_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.690083\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.07573627 0.07464051 0.07543179 0.07370009]\n",
      "Training resnet for 0.075% of train size (aka 6261 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/126 [>.............................] - ETA: 1:23 - loss: 1.3738 - accuracy: 0.3219WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1031s vs `on_train_batch_begin` time: 0.1405s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1031s vs `on_train_batch_end` time: 0.3632s). Check your callbacks.\n",
      "126/126 [==============================] - 35s 227ms/step - loss: 1.2683 - accuracy: 0.4337 - val_loss: 1.4891 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "126/126 [==============================] - 25s 197ms/step - loss: 1.2205 - accuracy: 0.4528 - val_loss: 1.5294 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "126/126 [==============================] - 25s 198ms/step - loss: 1.2434 - accuracy: 0.4303 - val_loss: 1.5867 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2304 - accuracy: 0.4455 - val_loss: 1.5746 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2285 - accuracy: 0.4510 - val_loss: 1.5597 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2208 - accuracy: 0.4565 - val_loss: 1.5520 - val_accuracy: 0.2500\n",
      "Epoch 7/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2248 - accuracy: 0.4473 - val_loss: 1.5560 - val_accuracy: 0.2500\n",
      "Epoch 8/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2294 - accuracy: 0.4423 - val_loss: 1.5671 - val_accuracy: 0.2500\n",
      "Epoch 9/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2306 - accuracy: 0.4364 - val_loss: 1.5564 - val_accuracy: 0.2500\n",
      "Epoch 10/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2367 - accuracy: 0.4417 - val_loss: 1.5609 - val_accuracy: 0.2500\n",
      "Epoch 11/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2219 - accuracy: 0.4458 - val_loss: 1.5530 - val_accuracy: 0.2500\n",
      "Epoch 12/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2407 - accuracy: 0.4370 - val_loss: 1.5627 - val_accuracy: 0.2500\n",
      "Epoch 13/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2323 - accuracy: 0.4431 - val_loss: 1.5552 - val_accuracy: 0.2500\n",
      "Epoch 14/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2379 - accuracy: 0.4365 - val_loss: 1.5674 - val_accuracy: 0.2500\n",
      "Epoch 15/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2220 - accuracy: 0.4374 - val_loss: 1.5557 - val_accuracy: 0.2500\n",
      "Epoch 16/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2218 - accuracy: 0.4498 - val_loss: 1.5527 - val_accuracy: 0.2500\n",
      "Epoch 17/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2345 - accuracy: 0.4458 - val_loss: 1.5651 - val_accuracy: 0.2500\n",
      "Epoch 18/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2290 - accuracy: 0.4516 - val_loss: 1.5588 - val_accuracy: 0.2500\n",
      "Epoch 19/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2212 - accuracy: 0.4523 - val_loss: 1.5542 - val_accuracy: 0.2500\n",
      "Epoch 20/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2391 - accuracy: 0.4359 - val_loss: 1.5604 - val_accuracy: 0.2500\n",
      "Epoch 21/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2322 - accuracy: 0.4356 - val_loss: 1.5604 - val_accuracy: 0.2500\n",
      "Epoch 22/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2288 - accuracy: 0.4479 - val_loss: 1.5571 - val_accuracy: 0.2500\n",
      "Epoch 23/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2149 - accuracy: 0.4389 - val_loss: 1.5568 - val_accuracy: 0.2500\n",
      "Epoch 24/30\n",
      "126/126 [==============================] - 25s 198ms/step - loss: 1.2233 - accuracy: 0.4505 - val_loss: 1.5570 - val_accuracy: 0.2500\n",
      "Epoch 25/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2311 - accuracy: 0.4428 - val_loss: 1.5586 - val_accuracy: 0.2500\n",
      "Epoch 26/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2397 - accuracy: 0.4377 - val_loss: 1.5618 - val_accuracy: 0.2500\n",
      "Epoch 27/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2315 - accuracy: 0.4426 - val_loss: 1.5610 - val_accuracy: 0.2500\n",
      "Epoch 28/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2238 - accuracy: 0.4417 - val_loss: 1.5597 - val_accuracy: 0.2500\n",
      "Epoch 29/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2333 - accuracy: 0.4401 - val_loss: 1.5589 - val_accuracy: 0.2500\n",
      "Epoch 30/30\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 1.2199 - accuracy: 0.4472 - val_loss: 1.5585 - val_accuracy: 0.2500\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_6261_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.250000\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.09158275 0.08899341 0.08970744 0.08983287]\n",
      "Training resnet for 0.09% of train size (aka 7513 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/151 [>.............................] - ETA: 1:37 - loss: 1.3773 - accuracy: 0.2498WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1018s vs `on_train_batch_begin` time: 0.1379s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1018s vs `on_train_batch_end` time: 0.3440s). Check your callbacks.\n",
      "151/151 [==============================] - 39s 221ms/step - loss: 1.2639 - accuracy: 0.4194 - val_loss: 1.5011 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "151/151 [==============================] - 30s 197ms/step - loss: 1.2271 - accuracy: 0.4451 - val_loss: 1.5388 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2319 - accuracy: 0.4436 - val_loss: 1.5554 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2353 - accuracy: 0.4431 - val_loss: 1.5573 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2252 - accuracy: 0.4418 - val_loss: 1.5350 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2328 - accuracy: 0.4450 - val_loss: 1.5666 - val_accuracy: 0.2500\n",
      "Epoch 7/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2343 - accuracy: 0.4420 - val_loss: 1.5568 - val_accuracy: 0.25002344 \n",
      "Epoch 8/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2304 - accuracy: 0.4420 - val_loss: 1.5605 - val_accuracy: 0.2500\n",
      "Epoch 9/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2286 - accuracy: 0.4426 - val_loss: 1.5623 - val_accuracy: 0.2500\n",
      "Epoch 10/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2376 - accuracy: 0.4346 - val_loss: 1.5620 - val_accuracy: 0.2500\n",
      "Epoch 11/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2194 - accuracy: 0.4557 - val_loss: 1.5530 - val_accuracy: 0.2500\n",
      "Epoch 12/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2311 - accuracy: 0.4352 - val_loss: 1.5501 - val_accuracy: 0.2500\n",
      "Epoch 13/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2346 - accuracy: 0.4390 - val_loss: 1.5583 - val_accuracy: 0.2500\n",
      "Epoch 14/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2294 - accuracy: 0.4475 - val_loss: 1.5586 - val_accuracy: 0.2500\n",
      "Epoch 15/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2328 - accuracy: 0.4384 - val_loss: 1.5502 - val_accuracy: 0.2500\n",
      "Epoch 16/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2272 - accuracy: 0.4429 - val_loss: 1.5579 - val_accuracy: 0.2500\n",
      "Epoch 17/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2225 - accuracy: 0.4463 - val_loss: 1.5535 - val_accuracy: 0.2500\n",
      "Epoch 18/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2353 - accuracy: 0.4426 - val_loss: 1.5584 - val_accuracy: 0.2500\n",
      "Epoch 19/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2287 - accuracy: 0.4456 - val_loss: 1.5550 - val_accuracy: 0.2500\n",
      "Epoch 20/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2298 - accuracy: 0.4366 - val_loss: 1.5571 - val_accuracy: 0.2500\n",
      "Epoch 21/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2336 - accuracy: 0.4447 - val_loss: 1.5576 - val_accuracy: 0.2500\n",
      "Epoch 22/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2293 - accuracy: 0.4457 - val_loss: 1.5577 - val_accuracy: 0.2500\n",
      "Epoch 23/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2260 - accuracy: 0.4445 - val_loss: 1.5566 - val_accuracy: 0.2500\n",
      "Epoch 24/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2341 - accuracy: 0.4389 - val_loss: 1.5588 - val_accuracy: 0.2500\n",
      "Epoch 25/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2426 - accuracy: 0.4378 - val_loss: 1.5617 - val_accuracy: 0.2500\n",
      "Epoch 26/30\n",
      "151/151 [==============================] - 30s 198ms/step - loss: 1.2172 - accuracy: 0.4477 - val_loss: 1.5568 - val_accuracy: 0.2500\n",
      "Epoch 27/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2292 - accuracy: 0.4486 - val_loss: 1.5556 - val_accuracy: 0.2500\n",
      "Epoch 28/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2342 - accuracy: 0.4354 - val_loss: 1.5566 - val_accuracy: 0.2500\n",
      "Epoch 29/30\n",
      "151/151 [==============================] - 30s 198ms/step - loss: 1.2369 - accuracy: 0.4380 - val_loss: 1.5563 - val_accuracy: 0.2500\n",
      "Epoch 30/30\n",
      "151/151 [==============================] - 30s 199ms/step - loss: 1.2287 - accuracy: 0.4367 - val_loss: 1.5565 - val_accuracy: 0.2500\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_7514_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.250000\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.10089303 0.09931461 0.10028199 0.0998143 ]\n",
      "Training resnet for 0.1% of train size (aka 8348 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/167 [>.............................] - ETA: 1:42 - loss: 1.3734 - accuracy: 0.4524WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_begin` time: 0.1293s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.3294s). Check your callbacks.\n",
      "167/167 [==============================] - 43s 225ms/step - loss: 1.2640 - accuracy: 0.4463 - val_loss: 1.4998 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "167/167 [==============================] - 33s 198ms/step - loss: 1.2358 - accuracy: 0.4463 - val_loss: 1.5613 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 1.2352 - accuracy: 0.4425 - val_loss: 1.5607 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 1.2287 - accuracy: 0.4486 - val_loss: 1.5434 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 1.2284 - accuracy: 0.4396 - val_loss: 1.5506 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 1.2447 - accuracy: 0.4358 - val_loss: 1.5791 - val_accuracy: 0.2500\n",
      "Epoch 7/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 1.1932 - accuracy: 0.4873 - val_loss: 1.6339 - val_accuracy: 0.2812\n",
      "Epoch 8/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.8648 - accuracy: 0.7138 - val_loss: 1.8662 - val_accuracy: 0.2500\n",
      "Epoch 9/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.7779 - accuracy: 0.7248 - val_loss: 1.2868 - val_accuracy: 0.4375\n",
      "Epoch 10/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.7142 - accuracy: 0.7399 - val_loss: 1.2587 - val_accuracy: 0.5000\n",
      "Epoch 11/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.6771 - accuracy: 0.7586 - val_loss: 2.6500 - val_accuracy: 0.2500\n",
      "Epoch 12/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.6368 - accuracy: 0.7913 - val_loss: 1.9922 - val_accuracy: 0.3125\n",
      "Epoch 13/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.6077 - accuracy: 0.7977 - val_loss: 2.4449 - val_accuracy: 0.2812\n",
      "Epoch 14/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.5584 - accuracy: 0.8193 - val_loss: 0.9769 - val_accuracy: 0.5938\n",
      "Epoch 15/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.5289 - accuracy: 0.8265 - val_loss: 1.5754 - val_accuracy: 0.4375\n",
      "Epoch 16/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.4659 - accuracy: 0.8432 - val_loss: 1.0804 - val_accuracy: 0.6250\n",
      "Epoch 17/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.4619 - accuracy: 0.8517 - val_loss: 0.9665 - val_accuracy: 0.5938\n",
      "Epoch 18/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.4229 - accuracy: 0.8843 - val_loss: 0.5151 - val_accuracy: 0.9688\n",
      "Epoch 19/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.3816 - accuracy: 0.8960 - val_loss: 1.0084 - val_accuracy: 0.7188\n",
      "Epoch 20/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.3731 - accuracy: 0.9042 - val_loss: 1.3218 - val_accuracy: 0.5625\n",
      "Epoch 21/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.3542 - accuracy: 0.9148 - val_loss: 0.5443 - val_accuracy: 0.8438\n",
      "Epoch 22/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.3306 - accuracy: 0.9264 - val_loss: 0.6129 - val_accuracy: 0.8438\n",
      "Epoch 23/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.2880 - accuracy: 0.9417 - val_loss: 1.0457 - val_accuracy: 0.7188\n",
      "Epoch 24/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.2721 - accuracy: 0.9474 - val_loss: 0.8869 - val_accuracy: 0.7500\n",
      "Epoch 25/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.2288 - accuracy: 0.9659 - val_loss: 1.2689 - val_accuracy: 0.5312\n",
      "Epoch 26/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.2278 - accuracy: 0.9604 - val_loss: 0.7118 - val_accuracy: 0.8125\n",
      "Epoch 27/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.2105 - accuracy: 0.9649 - val_loss: 0.4447 - val_accuracy: 0.9375\n",
      "Epoch 28/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.1897 - accuracy: 0.9695 - val_loss: 0.2291 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "167/167 [==============================] - 33s 199ms/step - loss: 0.1943 - accuracy: 0.9681 - val_loss: 0.2664 - val_accuracy: 0.9688\n",
      "Epoch 30/30\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.1613 - accuracy: 0.9827 - val_loss: 0.4154 - val_accuracy: 0.9062\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_8348_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.942149\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.25251758 0.24784303 0.25484667 0.24524141]\n",
      "Training resnet for 0.25% of train size (aka 20871 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/418 [..............................] - ETA: 4:41 - loss: 1.3728 - accuracy: 0.4381WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_begin` time: 0.1568s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.3391s). Check your callbacks.\n",
      "418/418 [==============================] - 92s 206ms/step - loss: 1.2487 - accuracy: 0.4414 - val_loss: 1.5511 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 1.2279 - accuracy: 0.4484 - val_loss: 1.5712 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "418/418 [==============================] - 84s 200ms/step - loss: 1.2319 - accuracy: 0.4403 - val_loss: 1.5430 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "418/418 [==============================] - 83s 200ms/step - loss: 1.2342 - accuracy: 0.4395 - val_loss: 1.5661 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 1.1214 - accuracy: 0.5386 - val_loss: 1.4482 - val_accuracy: 0.4375\n",
      "Epoch 6/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.7417 - accuracy: 0.7322 - val_loss: 1.3125 - val_accuracy: 0.5625\n",
      "Epoch 7/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.6578 - accuracy: 0.7792 - val_loss: 0.7938 - val_accuracy: 0.7500\n",
      "Epoch 8/30\n",
      "418/418 [==============================] - 83s 198ms/step - loss: 0.6008 - accuracy: 0.7997 - val_loss: 0.6735 - val_accuracy: 0.7500\n",
      "Epoch 9/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.5144 - accuracy: 0.8331 - val_loss: 0.8576 - val_accuracy: 0.7188\n",
      "Epoch 10/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.4592 - accuracy: 0.8620 - val_loss: 1.6143 - val_accuracy: 0.4688\n",
      "Epoch 11/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.4312 - accuracy: 0.8732 - val_loss: 1.2334 - val_accuracy: 0.5938\n",
      "Epoch 12/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.3764 - accuracy: 0.8986 - val_loss: 0.2569 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.3547 - accuracy: 0.9055 - val_loss: 0.3072 - val_accuracy: 0.9375\n",
      "Epoch 14/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.3099 - accuracy: 0.9193 - val_loss: 0.2572 - val_accuracy: 0.9375\n",
      "Epoch 15/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.2865 - accuracy: 0.9273 - val_loss: 0.3523 - val_accuracy: 0.8750\n",
      "Epoch 16/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.2542 - accuracy: 0.9392 - val_loss: 0.1821 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.2343 - accuracy: 0.9428 - val_loss: 0.1649 - val_accuracy: 0.9688\n",
      "Epoch 18/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.2017 - accuracy: 0.9552 - val_loss: 0.1232 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.1875 - accuracy: 0.9558 - val_loss: 0.0943 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.1695 - accuracy: 0.9616 - val_loss: 0.1410 - val_accuracy: 0.9688\n",
      "Epoch 21/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.1572 - accuracy: 0.9648 - val_loss: 0.2310 - val_accuracy: 0.9375\n",
      "Epoch 22/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.1293 - accuracy: 0.9731 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.1274 - accuracy: 0.9740 - val_loss: 0.4598 - val_accuracy: 0.9062\n",
      "Epoch 24/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.1120 - accuracy: 0.9755 - val_loss: 0.0794 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.1005 - accuracy: 0.9800 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.0838 - accuracy: 0.9855 - val_loss: 0.1704 - val_accuracy: 0.9688\n",
      "Epoch 27/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.0780 - accuracy: 0.9873 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.0762 - accuracy: 0.9875 - val_loss: 0.1482 - val_accuracy: 0.9688\n",
      "Epoch 29/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.0633 - accuracy: 0.9911 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "418/418 [==============================] - 83s 199ms/step - loss: 0.0668 - accuracy: 0.9908 - val_loss: 0.0515 - val_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_20871_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.979339\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.40547216 0.3980379  0.40183292 0.38927577]\n",
      "Training resnet for 0.4% of train size (aka 33393 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/668 [..............................] - ETA: 7:19 - loss: 1.3733 - accuracy: 0.3255WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_begin` time: 0.1538s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.3282s). Check your callbacks.\n",
      "668/668 [==============================] - 141s 203ms/step - loss: 1.2403 - accuracy: 0.4389 - val_loss: 1.5441 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "668/668 [==============================] - 133s 200ms/step - loss: 1.2319 - accuracy: 0.4468 - val_loss: 1.6044 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "668/668 [==============================] - 133s 200ms/step - loss: 1.2285 - accuracy: 0.4397 - val_loss: 1.5513 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2308 - accuracy: 0.4379 - val_loss: 1.5556 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2321 - accuracy: 0.4398 - val_loss: 1.5496 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2282 - accuracy: 0.4431 - val_loss: 1.5394 - val_accuracy: 0.2500\n",
      "Epoch 7/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2262 - accuracy: 0.4456 - val_loss: 1.5476 - val_accuracy: 0.2500\n",
      "Epoch 8/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2241 - accuracy: 0.4478 - val_loss: 1.5686 - val_accuracy: 0.2500\n",
      "Epoch 9/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2281 - accuracy: 0.4419 - val_loss: 1.5391 - val_accuracy: 0.2500\n",
      "Epoch 10/30\n",
      "668/668 [==============================] - 133s 200ms/step - loss: 1.2296 - accuracy: 0.4400 - val_loss: 1.5561 - val_accuracy: 0.2500\n",
      "Epoch 11/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2292 - accuracy: 0.4462 - val_loss: 1.5552 - val_accuracy: 0.2500\n",
      "Epoch 12/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2298 - accuracy: 0.4442 - val_loss: 1.5629 - val_accuracy: 0.2500\n",
      "Epoch 13/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2254 - accuracy: 0.4476 - val_loss: 1.5533 - val_accuracy: 0.2500\n",
      "Epoch 14/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2291 - accuracy: 0.4452 - val_loss: 1.5657 - val_accuracy: 0.2500\n",
      "Epoch 15/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2247 - accuracy: 0.4446 - val_loss: 1.5638 - val_accuracy: 0.2500\n",
      "Epoch 16/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2301 - accuracy: 0.4436 - val_loss: 1.5652 - val_accuracy: 0.2500\n",
      "Epoch 17/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2301 - accuracy: 0.4412 - val_loss: 1.5592 - val_accuracy: 0.2500\n",
      "Epoch 18/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2259 - accuracy: 0.4430 - val_loss: 1.5654 - val_accuracy: 0.2500\n",
      "Epoch 19/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2304 - accuracy: 0.4407 - val_loss: 1.5649 - val_accuracy: 0.2500\n",
      "Epoch 20/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2286 - accuracy: 0.4469 - val_loss: 1.5556 - val_accuracy: 0.2500\n",
      "Epoch 21/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2247 - accuracy: 0.4458 - val_loss: 1.5574 - val_accuracy: 0.2500\n",
      "Epoch 22/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2231 - accuracy: 0.4446 - val_loss: 1.5550 - val_accuracy: 0.2500\n",
      "Epoch 23/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2276 - accuracy: 0.4443 - val_loss: 1.5583 - val_accuracy: 0.2500\n",
      "Epoch 24/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2258 - accuracy: 0.4469 - val_loss: 1.5628 - val_accuracy: 0.2500\n",
      "Epoch 25/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2242 - accuracy: 0.4466 - val_loss: 1.5713 - val_accuracy: 0.2500\n",
      "Epoch 26/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 1.2067 - accuracy: 0.4506 - val_loss: 1.4872 - val_accuracy: 0.2812\n",
      "Epoch 27/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 0.9785 - accuracy: 0.7175 - val_loss: 1.5053 - val_accuracy: 0.2812\n",
      "Epoch 28/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 0.8739 - accuracy: 0.7283 - val_loss: 2.0727 - val_accuracy: 0.2500\n",
      "Epoch 29/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 0.7926 - accuracy: 0.7373 - val_loss: 1.5731 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "668/668 [==============================] - 133s 199ms/step - loss: 0.7375 - accuracy: 0.7459 - val_loss: 1.0975 - val_accuracy: 0.5000\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_33394_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.496901\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.5065552  0.49565919 0.50361297 0.49396472]\n",
      "Training resnet for 0.5% of train size (aka 41742 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/835 [..............................] - ETA: 9:26 - loss: 1.3725 - accuracy: 0.4258 WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_begin` time: 0.1519s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.3450s). Check your callbacks.\n",
      "835/835 [==============================] - 175s 203ms/step - loss: 1.2389 - accuracy: 0.4390 - val_loss: 1.5582 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 1.2323 - accuracy: 0.4424 - val_loss: 1.5459 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 1.2290 - accuracy: 0.4442 - val_loss: 1.5340 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 1.2321 - accuracy: 0.4437 - val_loss: 1.5522 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 1.0695 - accuracy: 0.5654 - val_loss: 2.2044 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.6520 - accuracy: 0.7758 - val_loss: 1.4248 - val_accuracy: 0.4375\n",
      "Epoch 7/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.5333 - accuracy: 0.8294 - val_loss: 0.5079 - val_accuracy: 0.7500\n",
      "Epoch 8/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.4409 - accuracy: 0.8682 - val_loss: 1.5376 - val_accuracy: 0.5625\n",
      "Epoch 9/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.3879 - accuracy: 0.8882 - val_loss: 0.4804 - val_accuracy: 0.8125\n",
      "Epoch 10/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.3273 - accuracy: 0.9074 - val_loss: 1.4973 - val_accuracy: 0.5625\n",
      "Epoch 11/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.2726 - accuracy: 0.9227 - val_loss: 0.1869 - val_accuracy: 0.9688\n",
      "Epoch 12/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.2171 - accuracy: 0.9413 - val_loss: 0.1029 - val_accuracy: 0.9688\n",
      "Epoch 13/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.1932 - accuracy: 0.9468 - val_loss: 0.3028 - val_accuracy: 0.9062\n",
      "Epoch 14/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.1706 - accuracy: 0.9530 - val_loss: 0.0718 - val_accuracy: 0.9688\n",
      "Epoch 15/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.1483 - accuracy: 0.9604 - val_loss: 0.0949 - val_accuracy: 0.9688\n",
      "Epoch 16/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.1257 - accuracy: 0.9673 - val_loss: 0.1179 - val_accuracy: 0.9688\n",
      "Epoch 17/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.1040 - accuracy: 0.9741 - val_loss: 0.0538 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0922 - accuracy: 0.9768 - val_loss: 0.0923 - val_accuracy: 0.9688\n",
      "Epoch 19/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.0822 - accuracy: 0.9812 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.0729 - accuracy: 0.9829 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.0636 - accuracy: 0.9858 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "835/835 [==============================] - 167s 200ms/step - loss: 0.0548 - accuracy: 0.9883 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0528 - accuracy: 0.9882 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0403 - accuracy: 0.9927 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0414 - accuracy: 0.9927 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0373 - accuracy: 0.9939 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0328 - accuracy: 0.9947 - val_loss: 0.0985 - val_accuracy: 0.9688\n",
      "Epoch 28/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0307 - accuracy: 0.9950 - val_loss: 0.0695 - val_accuracy: 0.9688\n",
      "Epoch 29/30\n",
      "835/835 [==============================] - 166s 199ms/step - loss: 0.0285 - accuracy: 0.9955 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "835/835 [==============================] - 167s 199ms/step - loss: 0.0287 - accuracy: 0.9953 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_41742_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.988636\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.60516816 0.59567262 0.60327811 0.5985376 ]\n",
      "Training resnet for 0.6% of train size (aka 50090 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   6/1002 [..............................] - ETA: 11:47 - loss: 1.3724 - accuracy: 0.3322WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1024s vs `on_train_batch_begin` time: 0.1520s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1024s vs `on_train_batch_end` time: 0.3635s). Check your callbacks.\n",
      "1002/1002 [==============================] - 211s 203ms/step - loss: 1.2012 - accuracy: 0.4722 - val_loss: 1.9852 - val_accuracy: 0.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "1002/1002 [==============================] - 201s 200ms/step - loss: 0.7139 - accuracy: 0.7532 - val_loss: 0.8018 - val_accuracy: 0.7188\n",
      "Epoch 3/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.5740 - accuracy: 0.8101 - val_loss: 0.6035 - val_accuracy: 0.8438\n",
      "Epoch 4/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.4587 - accuracy: 0.8577 - val_loss: 0.4624 - val_accuracy: 0.8438\n",
      "Epoch 5/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.3797 - accuracy: 0.8851 - val_loss: 0.4567 - val_accuracy: 0.8750\n",
      "Epoch 6/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.3243 - accuracy: 0.9066 - val_loss: 0.8895 - val_accuracy: 0.7188\n",
      "Epoch 7/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.2605 - accuracy: 0.9264 - val_loss: 0.1370 - val_accuracy: 0.9688\n",
      "Epoch 8/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.2146 - accuracy: 0.9408 - val_loss: 0.1087 - val_accuracy: 0.9688\n",
      "Epoch 9/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.1947 - accuracy: 0.9464 - val_loss: 0.0598 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.1610 - accuracy: 0.9559 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.1380 - accuracy: 0.9632 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.1263 - accuracy: 0.9658 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.1088 - accuracy: 0.9717 - val_loss: 0.2199 - val_accuracy: 0.9375\n",
      "Epoch 14/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0962 - accuracy: 0.9756 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0850 - accuracy: 0.9796 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0702 - accuracy: 0.9840 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0605 - accuracy: 0.9868 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0527 - accuracy: 0.9890 - val_loss: 0.0594 - val_accuracy: 0.9688\n",
      "Epoch 19/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.0475 - accuracy: 0.9903 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0437 - accuracy: 0.9921 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0383 - accuracy: 0.9935 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0350 - accuracy: 0.9943 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0318 - accuracy: 0.9947 - val_loss: 0.0494 - val_accuracy: 0.9688\n",
      "Epoch 24/30\n",
      "1002/1002 [==============================] - 200s 200ms/step - loss: 0.0301 - accuracy: 0.9950 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0277 - accuracy: 0.9960 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0284 - accuracy: 0.9956 - val_loss: 0.0591 - val_accuracy: 0.9688\n",
      "Epoch 27/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0259 - accuracy: 0.9957 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0244 - accuracy: 0.9961 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0252 - accuracy: 0.9959 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "1002/1002 [==============================] - 200s 199ms/step - loss: 0.0240 - accuracy: 0.9961 - val_loss: 0.0295 - val_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_50090_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.988636\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.75500665 0.74748018 0.75255552 0.74222377]\n",
      "Training resnet for 0.75% of train size (aka 62613 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   6/1253 [..............................] - ETA: 15:06 - loss: 1.3739 - accuracy: 0.4279WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_begin` time: 0.1847s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.3485s). Check your callbacks.\n",
      "1253/1253 [==============================] - 259s 202ms/step - loss: 1.1481 - accuracy: 0.5065 - val_loss: 1.2420 - val_accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.6673 - accuracy: 0.7734 - val_loss: 1.1623 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "1253/1253 [==============================] - 249s 199ms/step - loss: 0.5137 - accuracy: 0.8376 - val_loss: 0.4819 - val_accuracy: 0.8438\n",
      "Epoch 4/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.3929 - accuracy: 0.8839 - val_loss: 0.2125 - val_accuracy: 0.9688\n",
      "Epoch 5/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.2893 - accuracy: 0.9163 - val_loss: 0.1081 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.2369 - accuracy: 0.9317 - val_loss: 0.4628 - val_accuracy: 0.7812\n",
      "Epoch 7/30\n",
      "1253/1253 [==============================] - 250s 200ms/step - loss: 0.2037 - accuracy: 0.9425 - val_loss: 0.0588 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.1780 - accuracy: 0.9496 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "1253/1253 [==============================] - 249s 199ms/step - loss: 0.1450 - accuracy: 0.9589 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.1228 - accuracy: 0.9661 - val_loss: 0.2083 - val_accuracy: 0.9375\n",
      "Epoch 11/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.1119 - accuracy: 0.9689 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0941 - accuracy: 0.9758 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0851 - accuracy: 0.9785 - val_loss: 0.2222 - val_accuracy: 0.9375\n",
      "Epoch 14/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0730 - accuracy: 0.9816 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0703 - accuracy: 0.9823 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0563 - accuracy: 0.9871 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0500 - accuracy: 0.9884 - val_loss: 0.0966 - val_accuracy: 0.9688\n",
      "Epoch 18/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0470 - accuracy: 0.9902 - val_loss: 0.1802 - val_accuracy: 0.9688\n",
      "Epoch 19/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0427 - accuracy: 0.9913 - val_loss: 0.1804 - val_accuracy: 0.9688\n",
      "Epoch 20/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0335 - accuracy: 0.9940 - val_loss: 0.0436 - val_accuracy: 0.9688\n",
      "Epoch 21/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0343 - accuracy: 0.9935 - val_loss: 0.6999 - val_accuracy: 0.8438\n",
      "Epoch 22/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0343 - accuracy: 0.9932 - val_loss: 0.0731 - val_accuracy: 0.9688\n",
      "Epoch 23/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0290 - accuracy: 0.9947 - val_loss: 0.1427 - val_accuracy: 0.9688\n",
      "Epoch 24/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0262 - accuracy: 0.9954 - val_loss: 0.1814 - val_accuracy: 0.9688\n",
      "Epoch 25/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0241 - accuracy: 0.9957 - val_loss: 0.1842 - val_accuracy: 0.9688\n",
      "Epoch 26/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0229 - accuracy: 0.9956 - val_loss: 0.1718 - val_accuracy: 0.9688\n",
      "Epoch 27/30\n",
      "1253/1253 [==============================] - 250s 200ms/step - loss: 0.0217 - accuracy: 0.9963 - val_loss: 0.1768 - val_accuracy: 0.9688\n",
      "Epoch 28/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0230 - accuracy: 0.9957 - val_loss: 0.1724 - val_accuracy: 0.9688\n",
      "Epoch 29/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0213 - accuracy: 0.9958 - val_loss: 0.1825 - val_accuracy: 0.9688\n",
      "Epoch 30/30\n",
      "1253/1253 [==============================] - 250s 199ms/step - loss: 0.0215 - accuracy: 0.9959 - val_loss: 0.1632 - val_accuracy: 0.9688\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_62613_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.993802\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [0.90378111 0.89694933 0.90192104 0.89902507]\n",
      "Training resnet for 0.9% of train size (aka 75135 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   6/1503 [..............................] - ETA: 18:27 - loss: 1.3743 - accuracy: 0.3892WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1034s vs `on_train_batch_begin` time: 0.1877s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1034s vs `on_train_batch_end` time: 0.3513s). Check your callbacks.\n",
      "1503/1503 [==============================] - 309s 202ms/step - loss: 1.2378 - accuracy: 0.4410 - val_loss: 1.5769 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "1503/1503 [==============================] - 300s 200ms/step - loss: 1.2313 - accuracy: 0.4433 - val_loss: 1.5764 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 1.2313 - accuracy: 0.4442 - val_loss: 1.5533 - val_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 1.2310 - accuracy: 0.4456 - val_loss: 1.5659 - val_accuracy: 0.2500\n",
      "Epoch 5/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 1.2223 - accuracy: 0.4519 - val_loss: 1.8902 - val_accuracy: 0.3125\n",
      "Epoch 6/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.6953 - accuracy: 0.7565 - val_loss: 0.6787 - val_accuracy: 0.7812\n",
      "Epoch 7/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.4526 - accuracy: 0.8603 - val_loss: 1.0873 - val_accuracy: 0.5000\n",
      "Epoch 8/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.3161 - accuracy: 0.9130 - val_loss: 0.1403 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.2323 - accuracy: 0.9363 - val_loss: 0.1949 - val_accuracy: 0.9375\n",
      "Epoch 10/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.1856 - accuracy: 0.9490 - val_loss: 0.1088 - val_accuracy: 0.9688\n",
      "Epoch 11/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.1552 - accuracy: 0.9564 - val_loss: 0.0679 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "1503/1503 [==============================] - 300s 200ms/step - loss: 0.1259 - accuracy: 0.9641 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.1076 - accuracy: 0.9695 - val_loss: 0.2269 - val_accuracy: 0.9062\n",
      "Epoch 14/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0891 - accuracy: 0.9756 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0765 - accuracy: 0.9805 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0649 - accuracy: 0.9840 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.0613 - accuracy: 0.9852 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0462 - accuracy: 0.9901 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0423 - accuracy: 0.9911 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.0344 - accuracy: 0.9934 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.0315 - accuracy: 0.9943 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "1503/1503 [==============================] - 300s 200ms/step - loss: 0.0305 - accuracy: 0.9942 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0272 - accuracy: 0.9948 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0240 - accuracy: 0.9956 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0233 - accuracy: 0.9956 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.0196 - accuracy: 0.9963 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "1503/1503 [==============================] - 299s 199ms/step - loss: 0.0195 - accuracy: 0.9958 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "1503/1503 [==============================] - 300s 199ms/step - loss: 0.0172 - accuracy: 0.9964 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_75136_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.993802\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels fraction: [1. 1. 1. 1.]\n",
      "Training resnet for 1.0% of train size (aka 83484 images) with pretrained: False and onlyLastLayersTrained: False...\n",
      "\n",
      "Total: 178 trainable: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   6/1670 [..............................] - ETA: 20:49 - loss: 1.3776 - accuracy: 0.2205WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.1198s vs `on_train_batch_begin` time: 0.1972s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1198s vs `on_train_batch_end` time: 0.3386s). Check your callbacks.\n",
      "1670/1670 [==============================] - 342s 202ms/step - loss: 1.1810 - accuracy: 0.4807 - val_loss: 1.5254 - val_accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a21c27d31b2a>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.6221 - accuracy: 0.7927 - val_loss: 0.7457 - val_accuracy: 0.7500\n",
      "Epoch 3/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.4941 - accuracy: 0.8472 - val_loss: 0.4734 - val_accuracy: 0.8438\n",
      "Epoch 4/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.3703 - accuracy: 0.8925 - val_loss: 0.3510 - val_accuracy: 0.8750\n",
      "Epoch 5/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.2503 - accuracy: 0.9295 - val_loss: 0.2356 - val_accuracy: 0.9062\n",
      "Epoch 6/30\n",
      "1670/1670 [==============================] - 333s 200ms/step - loss: 0.1941 - accuracy: 0.9422 - val_loss: 0.0813 - val_accuracy: 0.9688\n",
      "Epoch 7/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.1615 - accuracy: 0.9530 - val_loss: 0.1155 - val_accuracy: 0.9688\n",
      "Epoch 8/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.1383 - accuracy: 0.9595 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.1214 - accuracy: 0.9645 - val_loss: 0.0728 - val_accuracy: 0.9688\n",
      "Epoch 10/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.1036 - accuracy: 0.9695 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0909 - accuracy: 0.9736 - val_loss: 0.0571 - val_accuracy: 0.9688\n",
      "Epoch 12/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0796 - accuracy: 0.9773 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0702 - accuracy: 0.9796 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0585 - accuracy: 0.9832 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0505 - accuracy: 0.9860 - val_loss: 0.0738 - val_accuracy: 0.9688\n",
      "Epoch 16/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0466 - accuracy: 0.9877 - val_loss: 0.0399 - val_accuracy: 0.9688\n",
      "Epoch 17/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0373 - accuracy: 0.9904 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0330 - accuracy: 0.9914 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0288 - accuracy: 0.9923 - val_loss: 0.0310 - val_accuracy: 0.9688\n",
      "Epoch 20/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0243 - accuracy: 0.9939 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0229 - accuracy: 0.9943 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.0477 - val_accuracy: 0.9688\n",
      "Epoch 24/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0184 - accuracy: 0.9953 - val_loss: 0.0366 - val_accuracy: 0.9688\n",
      "Epoch 25/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0181 - accuracy: 0.9948 - val_loss: 0.0340 - val_accuracy: 0.9688\n",
      "Epoch 26/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0165 - accuracy: 0.9955 - val_loss: 0.0325 - val_accuracy: 0.9688\n",
      "Epoch 27/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0151 - accuracy: 0.9956 - val_loss: 0.0332 - val_accuracy: 0.9688\n",
      "Epoch 28/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0151 - accuracy: 0.9956 - val_loss: 0.0281 - val_accuracy: 0.9688\n",
      "Epoch 29/30\n",
      "1670/1670 [==============================] - 332s 199ms/step - loss: 0.0144 - accuracy: 0.9962 - val_loss: 0.0302 - val_accuracy: 0.9688\n",
      "Epoch 30/30\n",
      "1670/1670 [==============================] - 333s 199ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0368 - val_accuracy: 0.9688\n",
      "INFO:tensorflow:Assets written to: ../resnet/resnet_30epochs_83484_images_True_newWeights_False_lastLayerOnly_zerocenter_normalization/assets\n",
      "Test acc for resnet: 0.992769\n",
      "Done!\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "maxTrain = len(y_train)\n",
    "for newWeights in [True]:\n",
    "    tLayer = [False] if newWeights else [True, False] # do not train only last layer if new weights will be produced \n",
    "    for trainLastLayerOnly in tLayer:\n",
    "        for p in [0.01, 0.025, 0.05, 0.075, 0.09, 0.1, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9, 1.0]:\n",
    "            learning_rate = 0.1\n",
    "            momentum = 0.6\n",
    "            #X_trn, X_tst, y_trn, y_tst\n",
    "            if p < 1:\n",
    "                X_t, _, y_t, _ = train_test_split(images, y_train, test_size=1-p, random_state=56789)\n",
    "            else:\n",
    "                X_t = images; y_t = y_train;\n",
    "            print(f\"Labels fraction: {sum(y_t == 1) / sum(y_train == 1)}\")\n",
    "            for net in [\"resnet\"]:\n",
    "                print(f\"Training {net} for {p}% of train size (aka {len(X_t)} images) with pretrained: {not newWeights} and onlyLastLayersTrained: {trainLastLayerOnly}...\")\n",
    "                model, size, normalization = emptyModelGenerator(net, newWeights=newWeights, lastOnly=trainLastLayerOnly)\n",
    "                printTrainableLayers(model) # see if model is really well trained\n",
    "                X_trn = resizeIms(X_t, size)\n",
    "                #X_trn = normalizeIms(X_trn, normalization=normalization)\n",
    "                X_val = resizeIms(x_val, size)\n",
    "                #X_val = normalizeIms(X_val, normalization=normalization)\n",
    "                log_dir = f\"logs/{net}/fit/{p}trainSet_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                optimizer = SGD(learning_rate=learning_rate, momentum=momentum) # Adam(learning_rate, amsgrad=amsgrad) #create new optimizers\n",
    "                time_callback = TimeHistory()\n",
    "                model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                hist = model.fit(X_trn, y_t, epochs=epochs, validation_data = (X_val, y_validation), batch_size=batch_size,\n",
    "                            shuffle=True, max_queue_size=20,\n",
    "                            use_multiprocessing=True, workers=5, \n",
    "                            callbacks=[CustomCallback(fraction=0.9, model=net), tensorboard_callback, time_callback])\n",
    "                trainTime = sum(time_callback.times)\n",
    "                model.save(f\"../{net}/{net}_{epochs}epochs_{p*maxTrain:.0f}_images_{newWeights}_newWeights_{trainLastLayerOnly}_lastLayerOnly_{normalization}_normalization\")\n",
    "                tstAcc = testPredict(model, size, name=net) #, normalization=normalization)\n",
    "                results = results.append({\n",
    "                    'model': net, \n",
    "                    'train set images': len(X_t), \n",
    "                    'pretrained': not newWeights, \n",
    "                    'pretrained dataset': None if newWeights or net == \"octnet\" else (\"OCT2017,Srinivasan2014\" if net == \"opticnet\" else \"Imagenet\"),\n",
    "                    'Trained layers': \"Last dense\" if trainLastLayerOnly else \"All\", \n",
    "                    'epochs': epochs, \n",
    "                    'batch size': batch_size, \n",
    "                    'learning rate': f\"{learning_rate}_{momentum}\" if optimizer._name == \"SGD\" else learning_rate, \n",
    "                    'optimizer': \"Amsgrad\" if (optimizer._name == \"Adam\" and amsgrad) else optimizer._name,\n",
    "                    'training time (seconds)': trainTime, \n",
    "                    'train accuracy': hist.history[\"accuracy\"][-1], \n",
    "                    'train loss': hist.history[\"loss\"][-1],\n",
    "                    'validation accuracy': hist.history[\"val_accuracy\"][-1], \n",
    "                    'validation loss': hist.history[\"val_loss\"][-1], \n",
    "                    'test accuracy': tstAcc,\n",
    "                    #'normalization': normalization\n",
    "                }, ignore_index=True)\n",
    "                results.to_csv(\"Results_temp.csv\", index=False)\n",
    "                del model\n",
    "                del X_trn\n",
    "                del X_val\n",
    "                print(\"Done!\\n\" + '-'*50, end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
