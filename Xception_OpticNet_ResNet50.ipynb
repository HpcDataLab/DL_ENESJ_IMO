{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "#session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model_memory_usage_in_bytes(model, *, batch_size: int):\n",
    "    \"\"\"\n",
    "    Return the estimated memory usage of a given Keras model in bytes.\n",
    "    This includes the model weights and layers, but excludes the dataset.\n",
    "\n",
    "    The model shapes are multipled by the batch size, but the weights are not.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras model.\n",
    "        batch_size: The batch size you intend to run the model with. If you\n",
    "            have already specified the batch size in the model itself, then\n",
    "            pass `1` as the argument here.\n",
    "    Returns:\n",
    "        An estimate of the Keras model's memory usage in bytes.\n",
    "\n",
    "    \"\"\"\n",
    "    default_dtype = tf.keras.backend.floatx()\n",
    "    shapes_mem_count = 0\n",
    "    internal_model_mem_count = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.Model):\n",
    "            internal_model_mem_count += keras_model_memory_usage_in_bytes(\n",
    "                layer, batch_size=batch_size\n",
    "            )\n",
    "        single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n",
    "        out_shape = layer.output_shape\n",
    "        if isinstance(out_shape, list):\n",
    "            out_shape = out_shape[0]\n",
    "        for s in out_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = sum(\n",
    "        [tf.keras.backend.count_params(p) for p in model.trainable_weights]\n",
    "    )\n",
    "    non_trainable_count = sum(\n",
    "        [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\n",
    "    )\n",
    "\n",
    "    total_memory = (\n",
    "        batch_size * shapes_mem_count\n",
    "        + internal_model_mem_count\n",
    "        + trainable_count\n",
    "        + non_trainable_count\n",
    "    )\n",
    "    return total_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import models from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"../../data/OCT/OCT2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Classes Detected : 4\n"
     ]
    }
   ],
   "source": [
    "labels_available = os.listdir(os.path.join(dataPath, \"train\"))\n",
    "print(\"Total Number of Classes Detected :\",len(labels_available))\n",
    "\n",
    "labels_list = ['NORMAL',\"CNV\",\"DME\",\"DRUSEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL CNV DME DRUSEN "
     ]
    }
   ],
   "source": [
    "images=[]\n",
    "y_trn=[]\n",
    "for x in labels_list:\n",
    "    xPath = os.path.join(dataPath, \"train\", x)\n",
    "    myPicList = os.listdir(xPath)\n",
    "    for y in myPicList:\n",
    "        images.append(cv2.imread(os.path.join(xPath, y)))\n",
    "        y_trn.append(labels_list.index(x))\n",
    "    print(x ,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL CNV DME DRUSEN "
     ]
    }
   ],
   "source": [
    "x_val=[]\n",
    "y_val=[]\n",
    "for x in labels_list:\n",
    "    xPath = os.path.join(dataPath, \"val\", x)\n",
    "    myPicList = os.listdir(xPath)\n",
    "    for y in myPicList:\n",
    "        x_val.append(cv2.imread(os.path.join(xPath, y)))\n",
    "        y_val.append(labels_list.index(x))\n",
    "    print(x ,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeIm(im, size):\n",
    "    if im.shape[2] == 1:\n",
    "        im = cv2.cvtColor(im,cv2.COLOR_GRAY2RGB)\n",
    "    return cv2.resize(im, size)\n",
    "\n",
    "def resizeIms(x, size):\n",
    "    return np.array(list(map(lambda im: resizeIm(im, size), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xceptionNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d1eb592ea6cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Xception model occupied: {keras_model_memory_usage_in_bytes(xceptionNet, batch_size=1) / 10**6} MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xceptionNet' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Xception model occupied: {keras_model_memory_usage_in_bytes(xceptionNet, batch_size=1) / 10**6} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for Xception\n",
    "Modify model to classifiy just the number of desired categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input, decode_predictions\n",
    "\n",
    "# create the base pre-trained model\n",
    "xceptionNetModel = Xception(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xceptionNetModel.output\n",
    "# and a logistic layer -- let's say we have 4 classes\n",
    "predictions = Dense(len(labels_list), activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "xceptionNet = Model(inputs=xceptionNetModel.input, outputs=predictions)\n",
    "#xceptionNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize images to desired shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xceptionSize = (299, 299)\n",
    "X_train = resizeIms(images, xceptionSize)\n",
    "X_val = resizeIms(x_val, xceptionSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_trn,len(labels_list))\n",
    "y_validation = to_categorical(y_val,len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,fraction, model):\n",
    "        super(CustomCallback,self).__init__()\n",
    "        self.fraction = fraction\n",
    "        self.train_a = [];\n",
    "        self.val_a =[];\n",
    "        self.logPath = os.path.join(model, \"log.txt\")\n",
    "\n",
    "        if not os.path.isdir(model):\n",
    "            os.mkdir(model)\n",
    "\n",
    "        with open(self.logPath,'w') as f:\n",
    "            f.write('Starting of logging..\\n')\n",
    "\n",
    "        self.fig = plt.figure(figsize=(4,3))\n",
    "        self.ax = plt.subplot(1,1,1)\n",
    "        plt.ion()\n",
    "\n",
    "    def on_train_begin(self,logs=None):\n",
    "        self.fig.show()\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def on_train_end(self,logs=None):\n",
    "        with open(self.logPath,'a') as f:\n",
    "              f.write('End of logging..\\n')\n",
    "    \n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        lr= tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        lr *= self.fraction\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr,lr)\n",
    "        with open(self.logPath,'a') as f:\n",
    "            f.write('At epoch {:02d}, learning rate changed to {:.4f}\\n'.format(epoch,lr))\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        val_acc = logs.get('val_accuracy')\n",
    "        train_acc = logs.get('accuracy')\n",
    "        self.train_a.append(train_acc)\n",
    "        self.val_a.append(val_acc)\n",
    "        with open(self.logPath,'a') as f:\n",
    "            f.write('At epoch {:02d}, training accuracy: {:.3f}, validation accuracy: {:.3f}\\n'.format(epoch,train_acc,val_acc))\n",
    "        self.ax.clear()\n",
    "        self.ax.plot(range(1+epoch),self.train_a,label=\"Training\")\n",
    "        self.ax.plot(range(1+epoch),self.val_a,label=\"Validation\")\n",
    "        self.ax.set_xlabel('Epochs')\n",
    "        self.ax.set_ylabel('Accuracy')\n",
    "        self.ax.legend()\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xceptionNet.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "xceptionNet.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-797fd00c81c1>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/557 [..............................] - ETA: 20:40 - loss: 0.0252 - accuracy: 0.9967WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4409s vs `on_train_batch_end` time: 1.7970s). Check your callbacks.\n",
      "557/557 [==============================] - 1250s 2s/step - loss: 0.0294 - accuracy: 0.9948 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-797fd00c81c1>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/557 [===============>..............] - ETA: 9:20 - loss: 0.0280 - accuracy: 0.9951"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-02bd0f7294cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m xceptionHist = xceptionNet.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_validation), batch_size=150,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xceptionHist = xceptionNet.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_validation), batch_size=150,\n",
    "                    shuffle=True,\n",
    "                    max_queue_size=20,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=5,\n",
    "                   callbacks=[CustomCallback(fraction=0.9, model=\"Xception\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.9846301078796387,\n",
       "  0.5983363389968872,\n",
       "  0.4142909646034241,\n",
       "  0.31271785497665405,\n",
       "  0.24276559054851532,\n",
       "  0.19749096035957336,\n",
       "  0.16444873809814453,\n",
       "  0.13604038953781128,\n",
       "  0.11632142215967178,\n",
       "  0.09801001846790314,\n",
       "  0.08458360284566879,\n",
       "  0.07463981956243515,\n",
       "  0.06700371950864792,\n",
       "  0.06088167056441307,\n",
       "  0.05482383072376251,\n",
       "  0.05159606412053108,\n",
       "  0.04856733977794647,\n",
       "  0.045376040041446686,\n",
       "  0.04310394078493118,\n",
       "  0.04071763902902603,\n",
       "  0.039301056414842606,\n",
       "  0.037413157522678375,\n",
       "  0.035869620740413666,\n",
       "  0.03491583094000816,\n",
       "  0.033987730741500854,\n",
       "  0.032872606068849564,\n",
       "  0.032120879739522934,\n",
       "  0.03090815804898739,\n",
       "  0.030536150559782982,\n",
       "  0.030004046857357025],\n",
       " 'accuracy': [0.8528221249580383,\n",
       "  0.9322265386581421,\n",
       "  0.9479541182518005,\n",
       "  0.9561113715171814,\n",
       "  0.9638733267784119,\n",
       "  0.969491183757782,\n",
       "  0.9738872051239014,\n",
       "  0.9782713055610657,\n",
       "  0.9815653562545776,\n",
       "  0.9849791526794434,\n",
       "  0.9874826073646545,\n",
       "  0.9890996813774109,\n",
       "  0.9900580048561096,\n",
       "  0.9909324049949646,\n",
       "  0.9921062588691711,\n",
       "  0.992118239402771,\n",
       "  0.9924536347389221,\n",
       "  0.992968738079071,\n",
       "  0.9931124448776245,\n",
       "  0.9935317039489746,\n",
       "  0.9935077428817749,\n",
       "  0.9936993718147278,\n",
       "  0.9940108060836792,\n",
       "  0.9938670992851257,\n",
       "  0.9940826892852783,\n",
       "  0.9942623972892761,\n",
       "  0.9942384362220764,\n",
       "  0.9943821430206299,\n",
       "  0.994633674621582,\n",
       "  0.9945977926254272],\n",
       " 'val_loss': [0.9159892201423645,\n",
       "  0.5279166102409363,\n",
       "  0.3309137225151062,\n",
       "  0.23471665382385254,\n",
       "  0.2332642674446106,\n",
       "  0.134046733379364,\n",
       "  0.109444759786129,\n",
       "  0.1569434106349945,\n",
       "  0.07121530175209045,\n",
       "  0.05987672507762909,\n",
       "  0.050985150039196014,\n",
       "  0.0443110316991806,\n",
       "  0.03896395117044449,\n",
       "  0.03438812866806984,\n",
       "  0.031288936734199524,\n",
       "  0.03520872816443443,\n",
       "  0.03871150314807892,\n",
       "  0.049876678735017776,\n",
       "  0.039926253259181976,\n",
       "  0.03515292704105377,\n",
       "  0.0406004972755909,\n",
       "  0.044040530920028687,\n",
       "  0.03414732590317726,\n",
       "  0.03705684095621109,\n",
       "  0.03513612225651741,\n",
       "  0.03680970147252083,\n",
       "  0.037781767547130585,\n",
       "  0.03739554435014725,\n",
       "  0.034200917929410934,\n",
       "  0.03209158033132553],\n",
       " 'val_accuracy': [0.875,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xceptionHist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Xception/xception_30epochs_adam/assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "xceptionNet.save(\"Xception/xception_30epochs_adam\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#xception20Epochs = keras.models.load_model(\"Xception/xception_20epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opticNet = tf.keras.models.load_model('../Optic_net-4_classes-Kermany2018.hdf5')\n",
    "#opticNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opticNet.get_config().keys()\n",
    "opticNet.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpticNet model occupied: 842.014372 MB\n"
     ]
    }
   ],
   "source": [
    "#opticNet.summary()\n",
    "print(f\"OpticNet model occupied: {keras_model_memory_usage_in_bytes(opticNet, batch_size=1) / 10**6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-7bc63fe362a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresizeIms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.8/site-packages/tensorflow/python/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opticNetsize = (224, 224)\n",
    "\n",
    "X_train = resizeIms(images, opticNetsize)\n",
    "X_val = resizeIms(x_val, opticNetsize)\n",
    "\n",
    "y_train = to_categorical(y_trn,len(labels_list))\n",
    "y_validation = to_categorical(y_val,len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-797fd00c81c1>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   6/1670 [..............................] - ETA: 30:49 - loss: 7.5638 - accuracy: 0.0133WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4511s vs `on_train_batch_end` time: 0.6601s). Check your callbacks.\n",
      "1670/1670 [==============================] - 1934s 1s/step - loss: 0.1417 - accuracy: 0.9581 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-797fd00c81c1>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670/1670 [==============================] - 1931s 1s/step - loss: 0.0748 - accuracy: 0.9739 - val_loss: 0.0284 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1670/1670 [==============================] - 1930s 1s/step - loss: 0.0609 - accuracy: 0.9785 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1670/1670 [==============================] - 1931s 1s/step - loss: 0.0496 - accuracy: 0.9824 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1670/1670 [==============================] - 1943s 1s/step - loss: 0.0389 - accuracy: 0.9862 - val_loss: 0.0156 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opticHist = opticNet.fit(X_train, y_train, epochs=5, validation_data =(X_val, y_validation), batch_size=50,\n",
    "                    shuffle=True,\n",
    "                    max_queue_size=20,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=5,\n",
    "                   callbacks=[CustomCallback(fraction=0.9, model=\"OpticNet\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: OpticNet-71/opticnet_5epochs/assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "opticNet.save(\"OpticNet-71/opticnet_5epochs\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#opticNet5Epochs = keras.models.load_model(\"OpticNet-71/opticnet_5epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "resNetModel = ResNet50(weights='imagenet')\n",
    "#resNetModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resNetModel.output\n",
    "# and a logistic layer -- let's say we have 4 classes\n",
    "predictions = Dense(len(labels_list), activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "resNet = Model(inputs=resNetModel.input, outputs=predictions)\n",
    "#resNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 model occupied: 174.12158 MB\n"
     ]
    }
   ],
   "source": [
    "#resNet.summary()\n",
    "print(f\"ResNet50 model occupied: {keras_model_memory_usage_in_bytes(resNet, batch_size=1) / 10**6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (224, 224)\n",
    "\n",
    "X_train = resizeIms(images, size)\n",
    "X_val = resizeIms(x_val, size)\n",
    "\n",
    "y_train = to_categorical(y_trn,len(labels_list))\n",
    "y_validation = to_categorical(y_val,len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resNet.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-797fd00c81c1>:20: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/835 [..............................] - ETA: 5:03 - loss: 1.3685 - accuracy: 0.5497WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1574s vs `on_train_batch_end` time: 0.1739s). Check your callbacks.\n",
      "835/835 [==============================] - 338s 384ms/step - loss: 1.1285 - accuracy: 0.7371 - val_loss: 1.3419 - val_accuracy: 0.4062\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-797fd00c81c1>:48: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835/835 [==============================] - 320s 383ms/step - loss: 0.6913 - accuracy: 0.8415 - val_loss: 0.8842 - val_accuracy: 0.7188\n",
      "Epoch 3/30\n",
      "835/835 [==============================] - 319s 383ms/step - loss: 0.4793 - accuracy: 0.8913 - val_loss: 0.3519 - val_accuracy: 0.9688\n",
      "Epoch 4/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.3230 - accuracy: 0.9328 - val_loss: 0.3366 - val_accuracy: 0.9375\n",
      "Epoch 5/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.2455 - accuracy: 0.9471 - val_loss: 0.2220 - val_accuracy: 0.9688\n",
      "Epoch 6/30\n",
      "835/835 [==============================] - 319s 383ms/step - loss: 0.1962 - accuracy: 0.9554 - val_loss: 0.0877 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.1620 - accuracy: 0.9625 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.1370 - accuracy: 0.9678 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.1179 - accuracy: 0.9717 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.0974 - accuracy: 0.9770 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.0864 - accuracy: 0.9792 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.0740 - accuracy: 0.9832 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0634 - accuracy: 0.9854 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.0559 - accuracy: 0.9881 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "835/835 [==============================] - 320s 383ms/step - loss: 0.0509 - accuracy: 0.9892 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "835/835 [==============================] - 319s 383ms/step - loss: 0.0429 - accuracy: 0.9913 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0402 - accuracy: 0.9921 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0374 - accuracy: 0.9925 - val_loss: 0.0580 - val_accuracy: 0.9688\n",
      "Epoch 19/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0336 - accuracy: 0.9937 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0342 - accuracy: 0.9933 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0313 - accuracy: 0.9941 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0284 - accuracy: 0.9943 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0291 - accuracy: 0.9941 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0286 - accuracy: 0.9940 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0265 - accuracy: 0.9947 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0231 - accuracy: 0.9951 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0237 - accuracy: 0.9949 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0251 - accuracy: 0.9946 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0226 - accuracy: 0.9951 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "835/835 [==============================] - 319s 382ms/step - loss: 0.0219 - accuracy: 0.9951 - val_loss: 0.0205 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnetHist = resNet.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_validation), batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    max_queue_size=20,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=5,\n",
    "                   callbacks=[CustomCallback(fraction=0.9, model=\"ResNet\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Resnet50/resnet50_30epochs_adam/assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "resNet.save(\"../Resnet50/resnet50_30epochs_adam\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#resNet30Epochs = keras.models.load_model(\"Resnet50/resnet50_30epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL CNV DME DRUSEN "
     ]
    }
   ],
   "source": [
    "x_tst=[]\n",
    "y_tst=[]\n",
    "for x in labels_list:\n",
    "    xPath = os.path.join(dataPath, \"test\", x)\n",
    "    myPicList = os.listdir(xPath)\n",
    "    for y in myPicList:\n",
    "        x_tst.append(cv2.imread(os.path.join(xPath, y)))\n",
    "        y_tst.append(labels_list.index(x))\n",
    "    print(x ,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc for Xception net: 0.9990\n",
      "Test acc for OpticNet net: 0.9959\n",
      "Test acc for ResNet50 net: 0.9959\n"
     ]
    }
   ],
   "source": [
    "xceptionNet = tf.keras.models.load_model('../DL_ENESJ_IMO/Xception/xception_30epochs_adam')\n",
    "opticNet = tf.keras.models.load_model('../Xception/OpticNet-71/opticnet_5epochs/')\n",
    "for i, net in enumerate([xceptionNet, opticNet, resNet]):\n",
    "    if net == xceptionNet:\n",
    "        size = (299, 299)\n",
    "    else:\n",
    "        size = (224, 224)\n",
    "    X_test = resizeIms(x_tst, size)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(y_tst)\n",
    "    \n",
    "    prediction = net.predict(X_test)\n",
    "    preds = np.argmax(prediction, axis=1)\n",
    "    acc = sum(preds == Y_test) / len(Y_test)\n",
    "    \n",
    "    if i == 0:\n",
    "        netName = \"Xception\"\n",
    "    elif i == 1:\n",
    "        netName = \"OpticNet\"\n",
    "    elif i == 2:\n",
    "        netName = \"ResNet50\"\n",
    "    \n",
    "    print(f'Test acc for {netName} net: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
